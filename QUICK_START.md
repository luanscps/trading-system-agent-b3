# âš¡ QUICK START - Trading Agent B3 (15 minutos)

> Guia rÃ¡pido para colocar o agent rodando em 15 minutos

---

## ğŸš… Passo 1: Clone e Setup (2 min)

```bash
# Clone repositÃ³rio
git clone https://github.com/luanscps/trading-system-agent-b3.git
cd trading-system-agent-b3

# Crie venv
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instale dependÃªncias
pip install -r requirements.txt
```

---

## ğŸš… Passo 2: Baixe Modelos (5-10 min) âœ… NOMES CORRETOS

```bash
# Execute no servidor com Ollama (10.41.10.151)
ollama pull smollm2:1.7b-instruct-q4_K_M
ollama pull mistral:7b-instruct-q4_K_M
ollama pull deepseek-r1:8b                    # âœ… CORRETO!
ollama pull qwen2.5-coder:7b-instruct-q4_K_M
ollama pull gemma3:4b-it                      # âœ… CORRETO!
```

**Nomes que NÃƒO funcionam:**
- âŒ `deepseek-r1:7b-instruct-q4_K_M` (nÃ£o existe)
- âŒ `gemma3:4b` (use `gemma3:4b-it`)

---

## ğŸš… Passo 3: Configure (1 min)

```bash
# Copie template
cp .env.example .env

# Edite com seu BRAPI_TOKEN
nano .env
```

**Campos obrigatÃ³rios em .env:**
```bash
BRAPI_TOKEN=seu_token_aqui
OLLAMA_HOST=http://10.41.10.151:11434
```

---

## ğŸš… Passo 4: Rode! (2 min)

```bash
python -m src.main
```

**Output esperado:**
```
ğŸš€ Trading Agent iniciado!
ğŸ“ˆ Ollama conectado: 10.41.10.151:11434
ğŸ“ˆ 5 modelos carregados (17.5GB)
ğŸŒ AnÃ¡lisando PETR4...
  âœ… SmolLM2: Sentimento BULLISH (85% confianÃ§a)
  âœ… Mistral: MACD positivo, RSI 60
  âœ… DeepSeek: Sharpe Ratio 1.2
  âœ… Qwen2.5: Dados vÃ¡lidos
  âœ… Gemma3: Sentimento positivo
ğŸš€ RECOMENDAÃ‡ÃƒO: BUY (score: 0.87)
```

---

## ğŸ”Œ Testes RÃ¡pidos

### Verificar conexÃ£o Ollama

```bash
curl http://10.41.10.151:11434/api/tags
```

### Testar um modelo

```bash
ollama run deepseek-r1:8b "Qual Ã© o Sharpe Ratio ideal para trading?"
```

### Verificar Python

```bash
python -c "from src.models.ollama_models import OllamaModels; print('OK')"
```

---

## ğŸ› Erros Comuns

### âŒ "Model not found: deepseek-r1:7b-instruct-q4_K_M"

**Problema**: Nome errado do modelo  
**SoluÃ§Ã£o**: Use `deepseek-r1:8b` (sem -instruct-q4_K_M)

```bash
# âŒ ERRADO
ollama pull deepseek-r1:7b-instruct-q4_K_M

# âœ… CORRETO
ollama pull deepseek-r1:8b
```

### âŒ "Cannot connect to Ollama"

**Problema**: Ollama nÃ£o estÃ¡ rodando em 10.41.10.151:11434  
**SoluÃ§Ã£o**: 

```bash
# Verificar se estÃ¡ rodando
curl http://10.41.10.151:11434/api/tags

# Se nÃ£o responder, iniciar Ollama
ollama serve
```

### âŒ "Out of memory"

**Problema**: Modelos usam muita RAM  
**SoluÃ§Ã£o**: Usar modelos menores em .env

```bash
# Em .env
OLLAMA_MODEL_STANDARD=smollm2:1.7b-instruct-q4_K_M
OLLAMA_MODEL_REASONING=deepseek-r1:8b  # MÃ¡ximo 4.5GB
```

---

## âœ… PrÃ³ximos Passos

1. **Testar em simulado**: Deixe rodando 1 hora e veja os logs
2. **Ler documentaÃ§Ã£o completa**: Ver `README.md` para detalhes
3. **Deploy em produÃ§Ã£o**: Ver `DEPLOY.md` para Docker/SystemD
4. **Customizar**: Ajuste prompts e modelos em `config/`

---

## ğŸ“ˆ Tabela de ReferÃªncia Modelos

| Modelo | Comando Ollama Correto | Tamanho | Tempo |
|--------|------------------------|---------|-------|
| SmolLM2 1.7B | `ollama pull smollm2:1.7b-instruct-q4_K_M` | 1.5GB | 300ms |
| Mistral 7B | `ollama pull mistral:7b-instruct-q4_K_M` | 4GB | 1-2s |
| DeepSeek-R1 8B | `ollama pull deepseek-r1:8b` | 4.5GB | 2-3s |
| Qwen2.5 7B | `ollama pull qwen2.5-coder:7b-instruct-q4_K_M` | 5GB | 1-2s |
| Gemma 3 4B | `ollama pull gemma3:4b-it` | 2.5GB | 800ms |

---

**ParabÃ©ns! ğŸ‰ Seu Trading Agent estÃ¡ rodando!**

PrÃ³ximo: Ler `README.md` para detalhes completos
